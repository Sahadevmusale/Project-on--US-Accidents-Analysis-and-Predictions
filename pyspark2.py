# -*- coding: utf-8 -*-
"""PySpark2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19X_gzmUcf6pOMmQ4g46Qn3MzCUMS56Y2
"""

!pip install pyspark py4j

from pyspark.sql import SparkSession
from pyspark.sql.types import *

spark= SparkSession.builder.appName("test_pyspark").getOrCreate()

schema = StructType([
    StructField('ID', StringType(), True),
    StructField('Severity',IntegerType(), True),
    StructField('Side',StringType(), True),
    StructField('City',StringType(), True),
    StructField('County',StringType(), True),
    StructField('State',StringType(), True),
    StructField('Description',StringType(), True),
    StructField('Start_Time',DateType(), True),
    StructField('Weather_Timestamp',DateType(), True),
    StructField('Temperature(F)',DoubleType(), True),
    StructField('End_Time',DateType(), True),
    StructField('Weather_Condition',StringType(), True)
])

from google.colab import drive
drive.mount('/content/drive/')

!ls "/content/drive/MyDrive/google_colab/US_Accidents_Dec21_updated.csv"

path='/content/drive/MyDrive/google_colab/US_Accidents_Dec21_updated.csv'

import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, date_format,month,year,to_timestamp,hour

df = pd.read_csv(path)

df_US=df[['ID','Severity','Side','City','County','State','Description','Start_Time','Weather_Timestamp','Temperature(F)','End_Time','Weather_Condition']].copy()

df_US.head()

df_US.to_csv("sorted_data.csv",index=False)

df_US1=spark.read.format("csv").option("header","True").schema(schema).load('/content/sorted_data.csv')

df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/content/sorted_data.csv") \
    .select("ID", "Severity", "Side", "City","County","State","Description","Start_Time","Weather_Timestamp","Temperature(F)","End_Time","Weather_Condition")

df_US1= df_US1.withColumn("Date", to_date("Start_Time"))

df_US1 = df_US1.withColumn("DayOfWeek", date_format("Date", "E"))

df_US1 = df_US1.withColumn("Month", month("Date"))

df_US1 = df_US1.withColumn("Year", year("Date"))

df_US1= df_US1.withColumn("Timestamp", to_timestamp("Start_Time", "MM/dd/yyyy hh:mm:ss a"))

df_US1 = df_US1.withColumn("HourOfDay", hour("Timestamp"))

df_US1.registerTempTable("US")

spark.sql("select * from US").show()

"""**Q1)How many unique cities in accident dataset**"""

num_cities = df_US1.select('City').distinct().count()
print("Number of cities in the accident dataset:", num_cities)

"""**Q2)Which days of the week have the most accidents?**

"""

result = spark.sql("SELECT DayOfWeek, COUNT(*) as AccidentCount FROM US GROUP BY DayOfWeek ORDER BY AccidentCount DESC")
result.show()

"""**Q3)How many unique states in accident dataset**"""

num_states = df_US1.select('State').distinct().count()
print("Number of states in the accident dataset:", num_states)

"""**Q4)Which 5 states have the highest number of accidents?**"""

result=spark.sql("SELECT State, COUNT(*) as AccidentCount FROM US GROUP BY State ORDER BY AccidentCount DESC LIMIT 5")
result.show()
#california, Florida, Texas, Oregon, Virginia.

"""**Q5)Which months have the most accidents?**"""

result = spark.sql("SELECT Month, COUNT(*) as AccidentCount FROM US GROUP BY Month ORDER BY AccidentCount DESC")
result.show()

"""**Q6)Find the common causes of the accident**"""

most_common_causes = spark.sql("""
    SELECT `Description`, COUNT(*) AS `count`
    FROM US
    WHERE `Description` IS NOT NULL
    GROUP BY `Description`
    ORDER BY `count` DESC
""").limit(10).collect()

most_common_causes

"""**Q7)What is the trend of accidents year over year (decreasing/increasing)? - It is exponentiallyÂ increasing**"""

result = spark.sql("SELECT Year, COUNT(*) as AccidentCount FROM US GROUP BY Year ORDER BY Year ASC")
result.show()

"""**Q8)find the type of accidents that most occurred in terms of severity**"""

spark.sql("""
    SELECT `Severity`, `Description`, COUNT(*) AS `count`
    FROM US
    WHERE `Description` IS NOT NULL
    GROUP BY `Severity`, `Description`
    ORDER BY `count` DESC
""").limit(10).collect()

# print the results
#for row in most_severe_accidents:
   # print(f"{row.Severity}: {row.Description}: {row.count}")

severity_counts = df_US1.groupBy('Severity').count()

most_frequent_severity = severity_counts.orderBy(severity_counts['count'].desc())
most_frequent_severity.show()

"""**Q9)At what time of the day are the accidents most frequent?**"""

result = df.groupBy("HourOfDay").count()
result = result.orderBy("count", ascending=False)
result.show()

"""**Q10)which city have highest accident rate in US dataset?**"""

query = """
        SELECT City, COUNT(*) as TotalAccidents
        FROM US
        GROUP BY City
        ORDER BY TotalAccidents DESC
        LIMIT 1
        """

# execute the query and display the result
result = spark.sql(query)
result.show()